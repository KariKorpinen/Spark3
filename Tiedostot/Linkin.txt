Linking with Spark

    Scala
    Java
    Python

Spark 1.3.0 uses Scala 2.10. To write applications in Scala, 
you will need to use a compatible Scala version (e.g. 2.10.X).

To write a Spark application, you need to add a Maven dependency 
on Spark. Spark is available through Maven Central at:

groupId = org.apache.spark
artifactId = spark-core_2.10
version = 1.3.0

In addition, if you wish to access an HDFS cluster, you need to 
add a dependency on hadoop-client for your version of HDFS. 
Some common HDFS version tags are listed on the third party distributions page.

groupId = org.apache.hadoop
artifactId = hadoop-client
version = <your-hdfs-version>
